{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13893509,"sourceType":"datasetVersion","datasetId":8851494}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio triton transformers trl peft bitsandbytes accelerate datasets tokenizers pyarrow\nprint(\"‚úÖ All packages removed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:50:19.552560Z","iopub.execute_input":"2025-12-01T14:50:19.553351Z","iopub.status.idle":"2025-12-01T14:50:22.251598Z","shell.execute_reply.started":"2025-12-01T14:50:19.553326Z","shell.execute_reply":"2025-12-01T14:50:22.250684Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nFound existing installation: triton 2.1.0\nUninstalling triton-2.1.0:\n  Successfully uninstalled triton-2.1.0\nFound existing installation: transformers 4.40.2\nUninstalling transformers-4.40.2:\n  Successfully uninstalled transformers-4.40.2\nFound existing installation: trl 0.9.4\nUninstalling trl-0.9.4:\n  Successfully uninstalled trl-0.9.4\nFound existing installation: peft 0.10.0\nUninstalling peft-0.10.0:\n  Successfully uninstalled peft-0.10.0\nFound existing installation: bitsandbytes 0.43.1\nUninstalling bitsandbytes-0.43.1:\n  Successfully uninstalled bitsandbytes-0.43.1\nFound existing installation: accelerate 0.28.0\nUninstalling accelerate-0.28.0:\n  Successfully uninstalled accelerate-0.28.0\nFound existing installation: datasets 2.20.0\nUninstalling datasets-2.20.0:\n  Successfully uninstalled datasets-2.20.0\nFound existing installation: tokenizers 0.19.1\nUninstalling tokenizers-0.19.1:\n  Successfully uninstalled tokenizers-0.19.1\nFound existing installation: pyarrow 19.0.1\nUninstalling pyarrow-19.0.1:\n  Successfully uninstalled pyarrow-19.0.1\n‚úÖ All packages removed\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Install PyTorch 2.1.2 (compatible with Triton 2.1.0 and BitsAndBytes)\n!pip install -q torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n\n# Install Triton 2.1.0\n!pip install -q triton==2.1.0\n\n# Install data/model libraries\n!pip install -q pyarrow==19.0.1 datasets==2.20.0\n\n# Install transformers + training libraries\n!pip install -q transformers==4.40.2 tokenizers==0.19.1\n!pip install -q trl==0.9.4 peft==0.10.0 accelerate==0.28.0 bitsandbytes==0.43.1\n\nprint(\"‚úÖ All packages installed\")\nprint(\"‚ö†Ô∏è RESTART SESSION NOW!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:51:29.799336Z","iopub.execute_input":"2025-12-01T14:51:29.799977Z","iopub.status.idle":"2025-12-01T14:53:17.895798Z","shell.execute_reply.started":"2025-12-01T14:51:29.799943Z","shell.execute_reply":"2025-12-01T14:53:17.895008Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m484.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires tokenizers, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ All packages installed\n‚ö†Ô∏è RESTART SESSION NOW!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import IPython\nIPython.Application.instance().kernel.do_shutdown(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:53:23.139669Z","iopub.execute_input":"2025-12-01T14:53:23.140367Z","iopub.status.idle":"2025-12-01T14:53:23.146564Z","shell.execute_reply.started":"2025-12-01T14:53:23.140338Z","shell.execute_reply":"2025-12-01T14:53:23.145980Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'status': 'ok', 'restart': True}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import sys\nimport torch\nimport triton\nimport transformers\nimport trl\nimport peft\nimport bitsandbytes\n\nprint(\"=\" * 60)\nprint(\"ENVIRONMENT VERIFICATION\")\nprint(\"=\" * 60)\nprint(f\"Python: {sys.version.split()[0]}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Triton: {triton.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"TRL: {trl.__version__}\")\nprint(f\"PEFT: {peft.__version__}\")\nprint(f\"BitsAndBytes: {bitsandbytes.__version__}\")\nprint(\"=\" * 60)\n\n# Test critical imports\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\nprint(\"‚úÖ ALL IMPORTS SUCCESSFUL - NO ERRORS!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:54:00.280184Z","iopub.execute_input":"2025-12-01T14:54:00.280810Z","iopub.status.idle":"2025-12-01T14:54:18.814883Z","shell.execute_reply.started":"2025-12-01T14:54:00.280782Z","shell.execute_reply":"2025-12-01T14:54:18.814206Z"}},"outputs":[{"name":"stdout","text":"============================================================\nENVIRONMENT VERIFICATION\n============================================================\nPython: 3.11.13\nPyTorch: 2.1.2+cu121\nTriton: 2.1.0\nCUDA Available: True\nTransformers: 4.40.2\nTRL: 0.9.4\nPEFT: 0.10.0\nBitsAndBytes: 0.43.1\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"2025-12-01 14:54:08.959378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764600849.150224     288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764600849.202607     288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"‚úÖ ALL IMPORTS SUCCESSFUL - NO ERRORS!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === EDITABLE CONFIG ===\nconfig = {\n    \"model_name\": \"microsoft/Phi-3-mini-4k-instruct\",\n    # common possible dataset locations: tries these in order\n    \"dataset_candidates\": [\n        \"/kaggle/input/text-to-cypher-supply/text_to_cypher_10000.csv\",\n    ],\n    \"output_dir\": \"./phi3_qlora_output\",\n    \"final_model_dir\": \"./phi3_qlora_final\",\n    \"max_seq_length\": 2048,   # kept for clarity; tokenization will handle truncation\n    \"train_test_split\": 0.15,\n    \"num_epochs\": 2,\n    \"batch_size\": 2,\n    \"gradient_accumulation\": 8,\n    \"learning_rate\": 2e-4,\n    \"lora_r\": 64,\n    \"lora_alpha\": 32,\n    \"lora_dropout\": 0.1,\n    \"seed\": 42,\n}\nprint(\"Config summary:\")\nfor k,v in config.items():\n    print(f\"  {k}: {v}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, time, math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:05:29.842528Z","iopub.execute_input":"2025-12-01T15:05:29.842837Z","iopub.status.idle":"2025-12-01T15:05:29.852379Z","shell.execute_reply.started":"2025-12-01T15:05:29.842813Z","shell.execute_reply":"2025-12-01T15:05:29.851771Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def find_dataset(candidates):\n    for p in candidates:\n        if os.path.exists(p):\n            return p\n    return None\n\ndataset_file = find_dataset(config[\"dataset_candidates\"])\nif dataset_file is None:\n    print(\"No dataset found in common paths. Please upload your CSV and set `dataset_file` variable.\")\n    raise FileNotFoundError(\"Dataset CSV not found. Put it in /kaggle/input/ or current working dir.\")\nelse:\n    print(\"Using dataset:\", dataset_file)\n    config[\"dataset_file\"] = dataset_file\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:05:29.852982Z","iopub.execute_input":"2025-12-01T15:05:29.853208Z","iopub.status.idle":"2025-12-01T15:05:29.893751Z","shell.execute_reply.started":"2025-12-01T15:05:29.853192Z","shell.execute_reply":"2025-12-01T15:05:29.893037Z"}},"outputs":[{"name":"stdout","text":"Using dataset: /kaggle/input/text-to-cypher-supply/text_to_cypher_10000.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\nprint(\"BNB config prepared. compute dtype:\", bnb_config.bnb_4bit_compute_dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:05:57.083638Z","iopub.execute_input":"2025-12-01T15:05:57.084399Z","iopub.status.idle":"2025-12-01T15:05:57.089903Z","shell.execute_reply.started":"2025-12-01T15:05:57.084372Z","shell.execute_reply":"2025-12-01T15:05:57.089322Z"}},"outputs":[{"name":"stdout","text":"BNB config prepared. compute dtype: torch.float16\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"Loading model (this can take 30-90s depending on network/GPU)...\")\nt0 = time.time()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config[\"model_name\"],\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n    low_cpu_mem_usage=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"], trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"Loaded model in {time.time()-t0:.1f}s. Device map:\")\ntry:\n    print(model.hf_device_map)\nexcept Exception:\n    pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:06:17.294177Z","iopub.execute_input":"2025-12-01T15:06:17.294442Z","iopub.status.idle":"2025-12-01T15:06:54.730982Z","shell.execute_reply.started":"2025-12-01T15:06:17.294422Z","shell.execute_reply":"2025-12-01T15:06:54.730066Z"}},"outputs":[{"name":"stdout","text":"Loading model (this can take 30-90s depending on network/GPU)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9493b9ce881b4e7186d4251733d4a782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92380dbaf65144e8b9de976f22e0ebe1"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4815850cda1f4cc8a2a391021a72eea9"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a56f6622d2c40e88a074e31ff9bf88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9dfbb0a834c49dfb90e3c0a6696c85d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d34c43ad66b4edd943632727582d2df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc131d235cac4f4183bbdecdb960b483"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b7aaca28b404e6f92af56dc5936c189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf06db40a404f98a35e246c04f75790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08fc72beda7428aa2cca07b7e2223a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159456cc1b79442aa4ac7caba82e080b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d3ac2b4e2449a79f2a5b119d4cf2be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99004af8fa6448c188fb3199bc6c2937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15a282f419f4e6492cb8aa76a0f18e7"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Loaded model in 37.4s. Device map:\n{'': 0}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"Preparing model for k-bit training and attaching LoRA adapters...\")\nmodel = prepare_model_for_kbit_training(model)\n\nlora_cfg = LoraConfig(\n    r=config[\"lora_r\"],\n    lora_alpha=config[\"lora_alpha\"],\n    lora_dropout=config[\"lora_dropout\"],\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_cfg)\nprint(\"LoRA attached. Trainable params:\")\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:08:11.684874Z","iopub.execute_input":"2025-12-01T15:08:11.685699Z","iopub.status.idle":"2025-12-01T15:08:12.159169Z","shell.execute_reply.started":"2025-12-01T15:08:11.685672Z","shell.execute_reply":"2025-12-01T15:08:12.158512Z"}},"outputs":[{"name":"stdout","text":"Preparing model for k-bit training and attaching LoRA adapters...\nLoRA attached. Trainable params:\ntrainable params: 35,651,584 || all params: 3,856,731,136 || trainable%: 0.9243989986031528\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"Loading dataset with Hugging Face datasets...\")\nds = load_dataset(\"csv\", data_files=config[\"dataset_file\"])[\"train\"]\nprint(\"Total rows in CSV:\", len(ds))\nprint(\"Columns:\", ds.column_names)\n# Quick peek\nprint(ds[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:08:29.885299Z","iopub.execute_input":"2025-12-01T15:08:29.885627Z","iopub.status.idle":"2025-12-01T15:08:30.293041Z","shell.execute_reply.started":"2025-12-01T15:08:29.885602Z","shell.execute_reply":"2025-12-01T15:08:30.292421Z"}},"outputs":[{"name":"stdout","text":"Loading dataset with Hugging Face datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48968899b0954498af03711e07d420b4"}},"metadata":{}},{"name":"stdout","text":"Total rows in CSV: 10000\nColumns: ['instruction', 'cypher']\n{'instruction': 'Top 5 products by total ordered quantity.', 'cypher': 'MATCH (p:Product)<-[:ORDERS]-(o:CustomerOrder) WITH p, sum(o.quantity) AS qty RETURN p.name, qty ORDER BY qty DESC LIMIT 5;'}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"Splitting dataset...\")\nds = ds.train_test_split(test_size=config[\"train_test_split\"], seed=config[\"seed\"])\ntrain_ds = ds[\"train\"]\neval_ds = ds[\"test\"]\nprint(\"Train size:\", len(train_ds), \"Eval size:\", len(eval_ds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:08:46.427154Z","iopub.execute_input":"2025-12-01T15:08:46.427452Z","iopub.status.idle":"2025-12-01T15:08:46.442514Z","shell.execute_reply.started":"2025-12-01T15:08:46.427430Z","shell.execute_reply":"2025-12-01T15:08:46.441740Z"}},"outputs":[{"name":"stdout","text":"Splitting dataset...\nTrain size: 8500 Eval size: 1500\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# This assumes CSV has columns named 'instruction' and 'cypher'.\n# If your columns have different names, change the keys below.\n\nIN_COL = \"instruction\"\nOUT_COL = \"cypher\"\n\ndef format_for_phi3(batch):\n    texts = []\n    for inst, cy in zip(batch[IN_COL], batch[OUT_COL]):\n        # phi-3 chat-style template\n        txt = f\"<|user|>\\n{inst}<|end|>\\n<|assistant|>\\n{cy}<|end|>\"\n        texts.append(txt)\n    return {\"text\": texts}\n\n# apply\ntrain_ds = train_ds.map(format_for_phi3, batched=True, remove_columns=train_ds.column_names)\neval_ds = eval_ds.map(format_for_phi3, batched=True, remove_columns=eval_ds.column_names)\n\n# small shuffle for training\ntrain_ds = train_ds.shuffle(seed=config[\"seed\"])\nprint(\"Sample formatted text:\", train_ds[0][\"text\"][:240])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:09:01.387078Z","iopub.execute_input":"2025-12-01T15:09:01.387379Z","iopub.status.idle":"2025-12-01T15:09:01.512072Z","shell.execute_reply.started":"2025-12-01T15:09:01.387357Z","shell.execute_reply":"2025-12-01T15:09:01.511500Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a213672618ca4f2c87b2c18b28753d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72bf0cc5b1b47d09bd8ab72766d8f29"}},"metadata":{}},{"name":"stdout","text":"Sample formatted text: <|user|>\nShow orders delayed by more than 7 days.<|end|>\n<|assistant|>\nMATCH (o:CustomerOrder) WHERE o.delay > 7 RETURN o.id, o.delay ORDER BY o.delay DESC;<|end|>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=config[\"output_dir\"],\n    per_device_train_batch_size=config[\"batch_size\"],\n    per_device_eval_batch_size=config[\"batch_size\"],\n    gradient_accumulation_steps=config[\"gradient_accumulation\"],\n    num_train_epochs=config[\"num_epochs\"],\n    learning_rate=config[\"learning_rate\"],\n    optim=\"paged_adamw_8bit\",\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    weight_decay=0.01,\n\n    # FIXED LINE ‚Üì‚Üì‚Üì\n    evaluation_strategy=\"steps\",\n\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=3,\n    logging_steps=25,\n    logging_first_step=True,\n    report_to=\"none\",\n    bf16=torch.cuda.is_bf16_supported(),\n    fp16=not torch.cuda.is_bf16_supported(),\n    gradient_checkpointing=True,\n    load_best_model_at_end=True,\n    seed=config[\"seed\"],\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:11:20.398591Z","iopub.execute_input":"2025-12-01T15:11:20.399336Z","iopub.status.idle":"2025-12-01T15:11:20.405683Z","shell.execute_reply.started":"2025-12-01T15:11:20.399308Z","shell.execute_reply":"2025-12-01T15:11:20.404927Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def formatting_func(example):\n    # SFTTrainer expects a function that returns text string(s)\n    return example[\"text\"]\n\n# attach tokenizer on the model as a fallback (some older code expects model.tokenizer)\nmodel.tokenizer = tokenizer\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    formatting_func=formatting_func,\n    args=training_args,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\nprint(\"SFTTrainer created.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:11:51.323569Z","iopub.execute_input":"2025-12-01T15:11:51.324098Z","iopub.status.idle":"2025-12-01T15:11:52.202454Z","shell.execute_reply.started":"2025-12-01T15:11:51.324072Z","shell.execute_reply":"2025-12-01T15:11:52.201766Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1910: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b112906cff9944e2b5da6bfb0b9aefb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"750f1a53ae2940e390b0c63629d83e03"}},"metadata":{}},{"name":"stdout","text":"SFTTrainer created.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\nt0 = time.time()\ntrainer.train()\nprint(\"Total training time (min):\", (time.time() - t0)/60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:12:07.621236Z","iopub.execute_input":"2025-12-01T15:12:07.621871Z","iopub.status.idle":"2025-12-01T18:32:07.137737Z","shell.execute_reply.started":"2025-12-01T15:12:07.621846Z","shell.execute_reply":"2025-12-01T18:32:07.137141Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nYou are not running the flash-attention implementation, expect numerical differences.\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nSTARTING TRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1062' max='1062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1062/1062 3:19:50, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.045000</td>\n      <td>0.042861</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.040600</td>\n      <td>0.043898</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.040600</td>\n      <td>0.041028</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.040400</td>\n      <td>0.040392</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.039800</td>\n      <td>0.040802</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.039300</td>\n      <td>0.039667</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.039700</td>\n      <td>0.038963</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.038600</td>\n      <td>0.039009</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.038100</td>\n      <td>0.038804</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.039000</td>\n      <td>0.038696</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Total training time (min): 199.99186536868413\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"Saving final model and tokenizer...\")\ntrainer.model.save_pretrained(config[\"final_model_dir\"])\ntokenizer.save_pretrained(config[\"final_model_dir\"])\nprint(\"Saved to:\", config[\"final_model_dir\"])\nprint(\"Contents:\", os.listdir(config[\"final_model_dir\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:32:30.852654Z","iopub.execute_input":"2025-12-01T18:32:30.852933Z","iopub.status.idle":"2025-12-01T18:32:31.361208Z","shell.execute_reply.started":"2025-12-01T18:32:30.852912Z","shell.execute_reply":"2025-12-01T18:32:31.360609Z"}},"outputs":[{"name":"stdout","text":"Saving final model and tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Saved to: ./phi3_qlora_final\nContents: ['special_tokens_map.json', 'added_tokens.json', 'adapter_config.json', 'tokenizer.model', 'adapter_model.safetensors', 'tokenizer_config.json', 'tokenizer.json', 'README.md']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import shutil\n\nsave_path = config[\"final_model_dir\"]        # example: \"/kaggle/working/phi3_qlora_final\"\nzip_path = \"/kaggle/working/phi3_qlora_final_zip\"\n\n# Create ZIP file\nshutil.make_archive(zip_path, 'zip', save_path)\n\nprint(\"Zipped model saved at:\", zip_path + \".zip\")\nprint(\"You can now download this file from the right sidebar ‚Üí Files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:34:04.061611Z","iopub.execute_input":"2025-12-01T18:34:04.061911Z","iopub.status.idle":"2025-12-01T18:34:11.148246Z","shell.execute_reply.started":"2025-12-01T18:34:04.061890Z","shell.execute_reply":"2025-12-01T18:34:11.147604Z"}},"outputs":[{"name":"stdout","text":"Zipped model saved at: /kaggle/working/phi3_qlora_final_zip.zip\nYou can now download this file from the right sidebar ‚Üí Files\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import zipfile\n\nzip_file = \"phi3_qlora_final_zip.zip\"\nextract_to = \"./phi3_qlora_final\"\n\nwith zipfile.ZipFile(zip_file, 'r') as zip_ref:\n    zip_ref.extractall(extract_to)\n\nprint(\"Extracted to:\", extract_to)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:35:37.849042Z","iopub.execute_input":"2025-12-01T18:35:37.849581Z","iopub.status.idle":"2025-12-01T18:35:38.921095Z","shell.execute_reply.started":"2025-12-01T18:35:37.849558Z","shell.execute_reply":"2025-12-01T18:35:38.920287Z"}},"outputs":[{"name":"stdout","text":"Extracted to: ./phi3_qlora_final\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\n\nbnb = BitsAndBytesConfig(load_in_4bit=True)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    quantization_config=bnb,\n    device_map=\"auto\"\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"./phi3_qlora_final\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"./phi3_qlora_final\")\n\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:36:10.099216Z","iopub.execute_input":"2025-12-01T18:36:10.100014Z","iopub.status.idle":"2025-12-01T18:36:26.714005Z","shell.execute_reply.started":"2025-12-01T18:36:10.099985Z","shell.execute_reply":"2025-12-01T18:36:26.713206Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"The repository for microsoft/Phi-3-mini-4k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-4k-instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\nThe repository for microsoft/Phi-3-mini-4k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-4k-instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"name":"stderr","text":"`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe701902d6f94310b1a541a66eca1dc6"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x Phi3DecoderLayer(\n            (self_attn): Phi3Attention(\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n              (rotary_emb): Phi3RotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm()\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm()\n          )\n        )\n        (norm): Phi3RMSNorm()\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"prompt = \"<|user|>Top 5 products by total ordered quantity.\\n<|end|>\\n<|assistant|>\\n\"\n\ntokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**tokens, max_new_tokens=100)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:41:22.838901Z","iopub.execute_input":"2025-12-01T18:41:22.839713Z","iopub.status.idle":"2025-12-01T18:41:26.090260Z","shell.execute_reply.started":"2025-12-01T18:41:22.839685Z","shell.execute_reply":"2025-12-01T18:41:26.089611Z"}},"outputs":[{"name":"stdout","text":"Top 5 products by total ordered quantity.\n MATCH (p:Product)<-[:ORDERS]-(o:CustomerOrder) WITH p, sum(o.quantity) AS qty RETURN p.name, qty ORDER BY qty DESC LIMIT 5;\n","output_type":"stream"}],"execution_count":30}]}