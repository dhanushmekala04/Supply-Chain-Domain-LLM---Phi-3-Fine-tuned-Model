{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":669048,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":506610,"modelId":521385}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install PyTorch 2.1.2 (compatible with Triton 2.1.0 and BitsAndBytes)\n!pip install -q torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n\n\n\n# Install data/model libraries\n!pip install -q pyarrow==19.0.1 datasets==2.20.0\n\n# Install transformers + training libraries\n!pip install -q transformers==4.40.2 tokenizers==0.19.1\n!pip install -q trl==0.9.4 peft==0.10.0 accelerate==0.28.0 bitsandbytes==0.43.1\n\nprint(\"✅ All packages installed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:00:25.785834Z","iopub.execute_input":"2025-12-01T20:00:25.786087Z","iopub.status.idle":"2025-12-01T20:02:47.502035Z","shell.execute_reply.started":"2025-12-01T20:00:25.786063Z","shell.execute_reply":"2025-12-01T20:02:47.501186Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m400.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.5.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h✅ All packages installed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\n\nbnb = BitsAndBytesConfig(load_in_4bit=True)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    quantization_config=bnb,\n    device_map=\"auto\"\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/fine-tune-phi/other/default/1\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/fine-tune-phi/other/default/1\")\n\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:06:35.744908Z","iopub.execute_input":"2025-12-01T20:06:35.745524Z","iopub.status.idle":"2025-12-01T20:06:48.366836Z","shell.execute_reply.started":"2025-12-01T20:06:35.745499Z","shell.execute_reply":"2025-12-01T20:06:48.366028Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"The repository for microsoft/Phi-3-mini-4k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-4k-instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\nThe repository for microsoft/Phi-3-mini-4k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-4k-instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"name":"stderr","text":"`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6848d61c2ae746f792139322cb3a47bc"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x Phi3DecoderLayer(\n            (self_attn): Phi3Attention(\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n              (rotary_emb): Phi3RotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm()\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm()\n          )\n        )\n        (norm): Phi3RMSNorm()\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"prompt = \"<|user|>Top 5 products by total ordered quantity.\\n<|end|>\\n<|assistant|>\\n\"\n\ntokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**tokens, max_new_tokens=100)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:07:23.417767Z","iopub.execute_input":"2025-12-01T20:07:23.418543Z","iopub.status.idle":"2025-12-01T20:07:26.771939Z","shell.execute_reply.started":"2025-12-01T20:07:23.418517Z","shell.execute_reply":"2025-12-01T20:07:26.771181Z"}},"outputs":[{"name":"stdout","text":"Top 5 products by total ordered quantity.\n MATCH (p:Product)<-[:ORDERS]-(o:CustomerOrder) WITH p, sum(o.quantity) AS qty RETURN p.name, qty ORDER BY qty DESC LIMIT 5;\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install transformers==4.40.2 peft==0.10.0 accelerate bitsandbytes\n!apt-get -y install git-lfs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:12:28.216046Z","iopub.execute_input":"2025-12-01T20:12:28.216784Z","iopub.status.idle":"2025-12-01T20:12:34.770219Z","shell.execute_reply.started":"2025-12-01T20:12:28.216758Z","shell.execute_reply":"2025-12-01T20:12:34.769462Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers==4.40.2 in /usr/local/lib/python3.11/dist-packages (4.40.2)\nRequirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (0.28.0)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.43.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.32.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (7.1.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (2.1.2+cu121)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.2) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.2) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.2) (2.4.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.6)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (2.1.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2025.10.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.2) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.2) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.40.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.40.2) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.40.2) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit-lfs is already the newest version (3.0.2-1ubuntu0.3).\n0 upgraded, 0 newly installed, 0 to remove and 165 not upgraded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"LORA_DIR = \"/kaggle/input/fine-tune-phi/other/default/1\"    # your uploaded folder\nBASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\nMERGED_DIR = \"/kaggle/working/phi3_merged_fp16\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:13:02.086933Z","iopub.execute_input":"2025-12-01T20:13:02.087688Z","iopub.status.idle":"2025-12-01T20:13:02.091539Z","shell.execute_reply.started":"2025-12-01T20:13:02.087651Z","shell.execute_reply":"2025-12-01T20:13:02.090957Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nprint(\"Loading base model...\")\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\",\n    trust_remote_code=True\n)\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n\nprint(\"Loading LoRA adapter:\", LORA_DIR)\npeft_model = PeftModel.from_pretrained(\n    base,\n    LORA_DIR,\n    torch_dtype=torch.float16\n)\n\nprint(\"Merging LoRA → base model...\")\nmerged = peft_model.merge_and_unload()\n\nprint(\"Saving merged FP16 model to:\", MERGED_DIR)\nmerged.save_pretrained(MERGED_DIR)\ntokenizer.save_pretrained(MERGED_DIR)\n\nprint(\"Merge complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:13:19.294768Z","iopub.execute_input":"2025-12-01T20:13:19.295671Z","iopub.status.idle":"2025-12-01T20:13:50.083758Z","shell.execute_reply.started":"2025-12-01T20:13:19.295626Z","shell.execute_reply":"2025-12-01T20:13:50.082998Z"}},"outputs":[{"name":"stdout","text":"Loading base model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a49b561e6504fe59f08a39faa83ba52"}},"metadata":{}},{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4131db407b0e44d3b5a19e310ebb2f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63afefe02b964a6fb56ba4542fcceb89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539398a81e0340c688e05950b1c0ab4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9bfd62ff174cf88c5a617cfb09b759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13693c0b0d774b15bd7b36f34616d69e"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Loading LoRA adapter: /kaggle/input/fine-tune-phi/other/default/1\nMerging LoRA → base model...\nSaving merged FP16 model to: /kaggle/working/phi3_merged_fp16\nMerge complete.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp /kaggle/working/llama.cpp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:29:14.412266Z","iopub.execute_input":"2025-12-01T20:29:14.412574Z","iopub.status.idle":"2025-12-01T20:29:31.135304Z","shell.execute_reply.started":"2025-12-01T20:29:14.412544Z","shell.execute_reply":"2025-12-01T20:29:31.134374Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Cloning into '/kaggle/working/llama.cpp'...\nremote: Enumerating objects: 70042, done.\u001b[K\nremote: Counting objects: 100% (260/260), done.\u001b[K\nremote: Compressing objects: 100% (164/164), done.\u001b[K\nremote: Total 70042 (delta 155), reused 96 (delta 96), pack-reused 69782 (from 2)\u001b[K\nReceiving objects: 100% (70042/70042), 214.40 MiB | 33.72 MiB/s, done.\nResolving deltas: 100% (50610/50610), done.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!ls /kaggle/working/llama.cpp\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:29:40.109374Z","iopub.execute_input":"2025-12-01T20:29:40.110168Z","iopub.status.idle":"2025-12-01T20:29:40.519869Z","shell.execute_reply.started":"2025-12-01T20:29:40.110123Z","shell.execute_reply":"2025-12-01T20:29:40.518709Z"}},"outputs":[{"name":"stdout","text":"AUTHORS\t\t\t       docs\t   pocs\nbenches\t\t\t       examples    poetry.lock\nbuild-xcframework.sh\t       flake.lock  pyproject.toml\nci\t\t\t       flake.nix   pyrightconfig.json\ncmake\t\t\t       ggml\t   README.md\nCMakeLists.txt\t\t       gguf-py\t   requirements\nCMakePresets.json\t       grammars    requirements.txt\nCODEOWNERS\t\t       include\t   scripts\ncommon\t\t\t       LICENSE\t   SECURITY.md\nCONTRIBUTING.md\t\t       licenses    src\nconvert_hf_to_gguf.py\t       Makefile    tests\nconvert_hf_to_gguf_update.py   media\t   tools\nconvert_llama_ggml_to_gguf.py  models\t   vendor\nconvert_lora_to_gguf.py        mypy.ini\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!ls -lh /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:34:06.393272Z","iopub.execute_input":"2025-12-01T20:34:06.393592Z","iopub.status.idle":"2025-12-01T20:34:06.790815Z","shell.execute_reply.started":"2025-12-01T20:34:06.393570Z","shell.execute_reply":"2025-12-01T20:34:06.789994Z"}},"outputs":[{"name":"stdout","text":"total 12K\ndrwxr-xr-x 25 root root 4.0K Dec  1 20:29 llama.cpp\n-rw-r--r--  1 root root   22 Dec  1 20:33 phi3_gguf_q4.zip\ndrwxr-xr-x  2 root root 4.0K Dec  1 20:13 phi3_merged_fp16\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"!python3 /kaggle/working/llama.cpp/convert_hf_to_gguf.py \\\n    /kaggle/working/phi3_merged_fp16 \\\n    --outfile /kaggle/working/phi3_gguf_q8_0.gguf \\\n    --outtype q8_0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:41:11.722126Z","iopub.execute_input":"2025-12-01T20:41:11.722478Z","iopub.status.idle":"2025-12-01T20:42:21.341426Z","shell.execute_reply.started":"2025-12-01T20:41:11.722445Z","shell.execute_reply":"2025-12-01T20:42:21.340677Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"INFO:hf-to-gguf:Loading model: phi3_merged_fp16\nINFO:hf-to-gguf:Model architecture: Phi3ForCausalLM\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00002.safetensors'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00002.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:output.weight,             torch.float16 --> Q8_0, shape = {3072, 32064}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.28.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.29.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.30.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.31.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:token_embd.weight,         torch.float16 --> Q8_0, shape = {3072, 32064}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> Q8_0, shape = {8192, 3072}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> Q8_0, shape = {3072, 16384}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {3072}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> Q8_0, shape = {3072, 3072}\nINFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float16 --> Q8_0, shape = {3072, 9216}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:gguf.vocab:Setting special token type bos to 1\nINFO:gguf.vocab:Setting special token type eos to 32000\nINFO:gguf.vocab:Setting special token type unk to 0\nINFO:gguf.vocab:Setting special token type pad to 32000\nINFO:gguf.vocab:Setting add_bos_token to False\nINFO:gguf.vocab:Setting add_eos_token to False\nINFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ eos_token }}{% endif %}\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:/kaggle/working/phi3_gguf_q8_0.gguf: n_tensors = 195, total_size = 4.1G\nWriting: 100%|██████████████████████████| 4.06G/4.06G [01:05<00:00, 62.3Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to /kaggle/working/phi3_gguf_q8_0.gguf\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"import shutil\n\nzip_path = \"/kaggle/working/phi3_gguf_q8_0.zip\"\n\nshutil.make_archive(zip_path.replace(\".zip\",\"\"), \"zip\", \"/kaggle/working\", \"phi3_gguf_q8_0.gguf\")\n\nprint(\"ZIP created:\", zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:44:19.631805Z","iopub.execute_input":"2025-12-01T20:44:19.632547Z","iopub.status.idle":"2025-12-01T20:47:17.338658Z","shell.execute_reply.started":"2025-12-01T20:44:19.632513Z","shell.execute_reply":"2025-12-01T20:47:17.337924Z"}},"outputs":[{"name":"stdout","text":"ZIP created: /kaggle/working/phi3_gguf_q8_0.zip\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/phi3_gguf_q8_0.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T20:51:20.852570Z","iopub.execute_input":"2025-12-01T20:51:20.853261Z","iopub.status.idle":"2025-12-01T20:51:20.858508Z","shell.execute_reply.started":"2025-12-01T20:51:20.853236Z","shell.execute_reply":"2025-12-01T20:51:20.857883Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/phi3_gguf_q8_0.zip","text/html":"<a href='/kaggle/working/phi3_gguf_q8_0.zip' target='_blank'>/kaggle/working/phi3_gguf_q8_0.zip</a><br>"},"metadata":{}}],"execution_count":52}]}